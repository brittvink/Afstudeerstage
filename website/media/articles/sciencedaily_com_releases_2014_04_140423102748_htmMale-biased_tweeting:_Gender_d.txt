A lot of Hollywood movies fail the Bechdel test miserably. That movies like "Star Wars" or "The Hobbit" do not fulfil the criteria is not surprising, considering the small number of female characters. However, even movies featuring many women, such as "Midnight in Paris," just pass the test by the skin of their teeth: "only because of a five-second dialogue between two women about a chair," says David Garcia, researcher at the Chair of Systems Design at ETH Zurich. There are disagreements, though, about whether the movie passes the test. Garcia and his colleagues from the Qatar Computing Research Institute have examined the gender bias of movies more closely. And for the first time, they have based their analysis not on expert ratings but on a computational approach. To this end, the scientists developed an algorithm that screened movie scripts for male-male and female-female dialogues and searched these dialogues for mentions of the respective other gender. The software thus computes interaction networks between characters and calculates a Bechdel score for female-female conversations that shows how often women talk to each other about something other than a man. In turn, the Bechdel score for male-male conversations shows how often men talk to each other about something other than a woman. The goal was to test not only whether a movie passes the Bechdel test but also to what degree it passes or fails. A script with 170,000 characters Taking their study further, the scientists used their algorithm to apply the Bechdel test to real-life conversations via the social media platform Twitter and to relate these to the gender bias of movies. They chose Twitter users from the US who shared the link to a movie trailer on YouTube over the course of six days in June 2013, plus the users who interacted with them over a longer period of time. The researchers analyzed the roughly 300 million tweets like a gigantic movie script with 170,000 characters to compute an interaction network. I expected that on Twitter men would mention women in their conversations as often as women mentioned men," says Garcia. The analysis revealed a different picture: Twitter conversations among men featured fewer mentions of women. In turn, there were more conversations between female Twitter users that contained references to men than conversations without a male reference. Students' tweets are unbiased The researchers did not find such a male bias in all Twitter users. The conversations of students proved to be more balanced regarding references to the respective other gender. In contrast, the tweets of fathers were even more male-biased: they interacted even less with female users and mentioned women even less often than childless men. "Possibly this is because fathers tend to be married while men without children may be married or single," Garcia suspects. Those Twitter users whose conversations passed the Bechdel test also tended to share the trailers of movies that also passed the test. In general, the trailers of such movies were shared less often via Twitter and received fewer positive ratings on YouTube than movies with a male bias, meaning they had fewer fans. It appears that Twitter is more male-biased," summarises Garcia. In comparison, conversations via Myspace, another social network, displayed less of a gender bias than those on Twitter, probably because conversations are more private on Myspace. Garcia's algorithm could serve as a tool not only to rate movies in which one of the genders is underrepresented, but also to analyze the design of social networks. The providers could thus test whether male or female users may be addressed to a lesser extent than intended. 